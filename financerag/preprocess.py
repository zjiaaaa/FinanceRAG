import os
import re
from typing import Dict, Any, List, Optional

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

# åªæœ‰é€™å…©å€‹ task è¦åš summarization
SUMM_TASKS = {"FinanceBench", "FinQABench"}

# BART large CNN æ‘˜è¦æ¨¡å‹
SUMM_MODEL = "facebook/bart-large-cnn"
# å¯ç”¨ç’°å¢ƒè®Šæ•¸é—œé–‰æ‘˜è¦ï¼Œé è¨­é–‹å•Ÿ
USE_SUMMARIZATION = os.getenv("USE_SUMM", "1").lower() not in {"0", "false", "off"}

_SUMMARIZER_CACHE = None

MAX_SUMM_CHARS = 2000      # æ‘˜è¦è¼¸å…¥æœ€å¤šä¿ç•™å¤šå°‘å­—å…ƒ
TARGET_SUMM_LEN = 128      # æ‘˜è¦è¼¸å‡ºé•·åº¦ä¸Šé™
MIN_SUMM_LEN = 40          # æ‘˜è¦è¼¸å‡ºé•·åº¦ä¸‹é™
FINANCIAL_TERMS = [
    "revenue", "sales", "net income", "operating income", "operating profit",
    "gross profit", "gross margin", "operating margin", "ebitda", "ebit",
    "eps", "earnings per share", "cash flow", "free cash flow", "dividend",
    "guidance", "forecast"
]

# å¸¸è¦‹çš„éé—œéµé¦–å­—å¤§å¯«å­—ï¼Œé¿å…å°‡ç–‘å•è©ç­‰è¦–ç‚ºå…¬å¸å
COMMON_CAP_STOPWORDS = {
    "what", "which", "when", "where", "who", "why", "how",
    "is", "are", "was", "were", "will", "do", "does", "did",
    "in", "on", "at", "for", "with", "from", "to", "of", "the", "a", "an"
}

# æ¸…æ´—æ–‡æœ¬
def clean_text(text: str) -> str:
    if not isinstance(text, str):
        return ""
    text = re.sub(r"<.*?>", " ", text)  # ç§»é™¤ HTML tags
    text = text.replace("\n", " ")       # æ›è¡Œ â†’ ç©ºç™½
    text = re.sub(r"\s+", " ", text)     # å¤šç©ºç™½åˆä½µ
    text = re.sub(r"[^\x00-\x7F]+", " ", text) 
    return text.strip()

# 10K / 10M / 3B è½‰æ›
def convert_money_unit(token: str):
    unit_map = {"K": 1_000, "M": 1_000_000, "B": 1_000_000_000}
    match = re.match(r"(\d+(?:\.\d+)?)([KMB])", token)
    if not match:
        return None
    number, unit = match.groups()
    return float(number) * unit_map[unit]

# æ—¥æœŸæŠ½å–
def extract_dates(text: str):
    pattern = r"\b\d{4}[-/.]\d{1,2}[-/.]\d{1,2}\b"
    return re.findall(pattern, text)

# K/M/B æŠ½å–
def extract_money_units(text: str):
    pattern = r"\b\d+(?:\.\d+)?[KMB]\b"
    return re.findall(pattern, text)

# ä¸€èˆ¬æ•¸å­—
def extract_plain_numbers(text: str, dates, numbers_raw):
    t = text
    for d in dates:
        t = t.replace(d, " ")
    for m in numbers_raw:
        t = t.replace(m, " ")
    t = re.sub(r"(\d),(?=\d{3})", r"\1", t)
    pattern = r"\d+(?:\.\d+)?"
    return re.findall(pattern, t)

# æ•¸å­—æ•´åˆ
def extract_all_numbers(clean_t: str):
    dates = extract_dates(clean_t)
    numbers_raw = extract_money_units(clean_t)

    numbers_value = []
    for token in numbers_raw:
        v = convert_money_unit(token)
        if v is not None:
            numbers_value.append(v)

    plain_numbers = extract_plain_numbers(clean_t, dates, numbers_raw)
    return dates, numbers_raw, numbers_value, plain_numbers

def extract_keywords(text: str) -> List[str]:
    if not text:
        return []

    keywords = []
    seen = set()
    lower_text = text.lower()

    def add_kw(token: str):
        key = token.strip().lower()
        if key and key not in seen:
            seen.add(key)
            keywords.append(key)

    # è‚¡ç¥¨ä»£ç¢¼æˆ–å…¨å¤§å¯«ç¸®å¯«
    for ticker in re.findall(r"\b[A-Z]{2,5}\b", text):
        add_kw(ticker)

    # å¯èƒ½çš„å…¬å¸åç¨± (å¤šå€‹é¦–å­—å¤§å¯«çš„è©çµ„)
    for phrase in re.findall(r"\b(?:[A-Z][a-zA-Z&]+(?:\s+[A-Z][a-zA-Z&]+)+)\b", text):
        add_kw(phrase)

    # å–®å€‹é¦–å­—å¤§å¯«è©
    for token in re.findall(r"\b[A-Z][a-zA-Z&]+\b", text):
        if token.lower() not in COMMON_CAP_STOPWORDS:
            add_kw(token)

    # å¹´ä»½
    for year in re.findall(r"\b(?:19|20)\d{2}\b", text):
        add_kw(year)

    # è²¡å‹™è¡“èª
    for term in FINANCIAL_TERMS:
        if term in lower_text:
            add_kw(term)

    return keywords

# chunk åˆ‡å‰²
def chunk_text(text: str, chunk_size: int = 300, overlap: int = 50) -> List[str]:
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = min(start + chunk_size, len(words))
        chunks.append(" ".join(words[start:end]))
        start += (chunk_size - overlap)
    return chunks

# MultiHiertt è¡¨æ ¼ â†’ è½‰æ–‡å­—
def flatten_multihiertt_table(table: Dict[str, Any]) -> str:
    """
    å°‡ MultiHiertt çš„éšå±¤å¼è¡¨æ ¼å±•å¹³ï¼š
    header = ["Category", "Item", "Value"]
    rows = [["Electronics","", ""], ["","iPhone","1200"]]
    æœƒè®Šæˆï¼š
    Category: Electronics
    Item: iPhone, Value: 1200
    """
    header = table.get("header", [])
    rows = table.get("rows", [])

    lines = []
    for row in rows:
        pairs = []
        for h, v in zip(header, row):
            if v and str(v).strip():     # é¿å…ç©ºå­—ä¸²
                pairs.append(f"{h}: {v}")
        if pairs:
            lines.append(", ".join(pairs))
    return "\n".join(lines)

def build_summarization_input(title: str, text: str) -> str:
    """
    å°‡ title + text åˆä½µå¾ŒåšåŸºæœ¬æˆªæ–·ï¼Œé¿å…å¤ªé•·ã€‚
    """
    t = " ".join([x for x in [title, text] if x]).strip()
    return t[:MAX_SUMM_CHARS]


def get_summarizer() -> Optional[Any]:
    """
    æ‡¶åŠ è¼‰æ‘˜è¦æ¨¡å‹ï¼›è‡ªå‹•åµæ¸¬ GPUï¼Œæ‰¾ä¸åˆ°å°±ç”¨ CPUã€‚
    """
    global _SUMMARIZER_CACHE

    if _SUMMARIZER_CACHE is not None:
        return _SUMMARIZER_CACHE

    if not USE_SUMMARIZATION:
        return None

    try:
        device = 0 if torch.cuda.is_available() else -1
        tokenizer = AutoTokenizer.from_pretrained(SUMM_MODEL)
        model = AutoModelForSeq2SeqLM.from_pretrained(SUMM_MODEL)
        _SUMMARIZER_CACHE = pipeline(
            "summarization",
            model=model,
            tokenizer=tokenizer,
            device=device,
        )
    except Exception as exc:
        print(f"[WARN] Summarizer init failed, skip summarization: {exc}")
        _SUMMARIZER_CACHE = None
    return _SUMMARIZER_CACHE


def summarize_for_task(task_name: str, doc_id: str, title: str, text: str) -> str:
    """
    åªåœ¨æŒ‡å®šçš„ task ä¸Šåš summarizationã€‚
    å¤±æ•—æ™‚å›å‚³ç©ºå­—ä¸²ï¼Œä¸å½±éŸ¿æ•´å€‹å‰è™•ç†æµç¨‹ã€‚
    """
    if (not USE_SUMMARIZATION) or (task_name not in SUMM_TASKS):
        return ""

    full = build_summarization_input(title, text)
    if not full:
        return ""

    summarizer = get_summarizer()
    if summarizer is None:
        return ""

    try:
        out = summarizer(
            full,
            max_length=TARGET_SUMM_LEN,
            min_length=MIN_SUMM_LEN,
            do_sample=False,
            truncation=True,
        )
        summ = out[0]["summary_text"]
        return clean_text(summ)
    except Exception as e:
        print(f"[WARN] Summarization failed for {task_name} doc {doc_id}: {e}")
        return ""

# æ•´åˆ MultiHiertt è¡¨æ ¼çš„ preprocess_task
def preprocess_task(task_name: str, task_obj: Any):

    for doc_id, doc in task_obj.corpus.items():

        text = doc.get("text", "")
        title = doc.get("title", "")

        clean_t = clean_text(text)
        clean_title = clean_text(title)

        if task_name == "MultiHiertt" and "table" in doc:
            table_text = flatten_multihiertt_table(doc["table"])
            doc["table_text"] = table_text

            # æŠŠè¡¨æ ¼æ–‡å­—ä½µå…¥ä¸»æ–‡æœ¬ï¼Œè®“ chunk èˆ‡ embedding éƒ½èƒ½ç”¨åˆ°
            clean_t = clean_t + "\n" + table_text

        # æ•¸å­—æŠ½å–
        dates, numbers_raw, numbers_value, plain_numbers = extract_all_numbers(clean_t)
        numbers_all = dates + numbers_raw + plain_numbers
        # ğŸ”¹ æ–°å¢ï¼šåªåœ¨ FinanceBench / FinQABench åš summarization
        summary_text = summarize_for_task(task_name, doc_id, clean_title, clean_t)
        if summary_text:
            # å­˜ä¸€ä»½æ‘˜è¦ç‰ˆæœ¬ï¼Œå¾ŒçºŒ embedding æƒ³ç”¨å°±ç”¨é€™å€‹æ¬„ä½
            doc["summary_text"] = summary_text
            # ä¹Ÿå¯ä»¥å¹«æ‘˜è¦åˆ‡ chunkï¼Œæ–¹ä¾¿å¾Œé¢é¸æ“‡ç”¨ summary_chunks
            doc["summary_chunks"] = chunk_text(summary_text, chunk_size=220, overlap=40)
        else:
            # æ²’åšæ‘˜è¦æˆ–å¤±æ•—ï¼Œå°±ä¸è¦æ”¾é€™å…©å€‹æ¬„ä½
            doc["summary_text"] = None
            doc["summary_chunks"] = []

        keywords = extract_keywords(f"{clean_title} {clean_t}")

        # chunk åˆ‡æ®µ
        chunks = chunk_text(clean_t, chunk_size=220, overlap=40)

        # å¯«å› corpus
        doc["text"] = clean_t
        doc["title"] = clean_title
        # æ‘˜è¦åœ¨å‰ã€åŸæ–‡åœ¨å¾Œï¼Œè®“ embedding / rerank éƒ½èƒ½ä½¿ç”¨æ‘˜è¦ç‰ˆæœ¬
        doc["chunks"] = (doc["summary_chunks"] or []) + chunks

        doc["dates"] = dates
        doc["numbers_raw"] = numbers_raw
        doc["numbers_value"] = numbers_value
        doc["numbers_plain"] = plain_numbers
        doc["numbers_all"] = numbers_all
        doc["keywords"] = keywords

    # Debug ç¬¬ä¸€ç­†
    first_doc_id = next(iter(task_obj.corpus))
    print(f"\n Debug {task_name} Document Sample")
    print(task_obj.corpus[first_doc_id])

    # è™•ç† queries
    new_queries = {}

    for qid, q_text in task_obj.queries.items():
        clean_q = clean_text(q_text)

        q_dates, q_numbers_raw, q_numbers_value, q_plain = extract_all_numbers(clean_q)
        q_numbers_all = q_dates + q_numbers_raw + q_plain
        q_keywords = extract_keywords(clean_q)

        new_queries[qid] = {
            "text": clean_q,
            "dates": q_dates,
            "numbers_raw": q_numbers_raw,
            "numbers_value": q_numbers_value,
            "numbers_plain": q_plain,
            "numbers_all": q_numbers_all,
            "keywords": q_keywords,
        }

    task_obj.queries = new_queries

    print(f"âœ” {task_name} å‰è™•ç†å®Œæˆï¼")
